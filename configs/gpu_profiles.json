{
  "profiles": {
    "B200": {
      "vram_mb": 184320,
      "compute_capability": "10.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 2048,
      "batch_size": 16,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": true,
      "tensor_cores": "5th_gen_blackwell",
      "notes": "Flagship B200 GPU - 180GB VRAM"
    },
    "H200_SXM": {
      "vram_mb": 144384,
      "compute_capability": "9.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1600,
      "batch_size": 12,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": true,
      "tensor_cores": "4th_gen_hopper",
      "notes": "H200 SXM - 141GB VRAM"
    },
    "MI300X": {
      "vram_mb": 196608,
      "compute_capability": "none",
      "default_quantization": "Q6_K",
      "fallback_quantization": "Q5_K_S",
      "max_resolution": 1024,
      "batch_size": 8,
      "sage_attention": "disabled",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "none",
      "notes": "AMD MI300X - 192GB VRAM"
    },
    "RTX_PRO_6000": {
      "vram_mb": 98304,
      "compute_capability": "8.9",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1600,
      "batch_size": 8,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "RTX Pro 6000 - 96GB VRAM"
    },
    "H100_NVL": {
      "vram_mb": 96256,
      "compute_capability": "9.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1600,
      "batch_size": 8,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": true,
      "tensor_cores": "4th_gen_hopper",
      "notes": "H100 NVL - 94GB VRAM"
    },
    "H100_SXM": {
      "vram_mb": 81920,
      "compute_capability": "9.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 8,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": true,
      "tensor_cores": "4th_gen_hopper",
      "notes": "H100 SXM - 80GB VRAM"
    },
    "H100_PCIe": {
      "vram_mb": 81920,
      "compute_capability": "9.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 8,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": true,
      "tensor_cores": "4th_gen_hopper",
      "notes": "H100 PCIe - 80GB VRAM"
    },
    "A100_PCIe": {
      "vram_mb": 81920,
      "compute_capability": "8.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "A100 PCIe - 80GB VRAM"
    },
    "A100_SXM": {
      "vram_mb": 81920,
      "compute_capability": "8.0",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "A100 SXM - 80GB VRAM"
    },
    "L40S": {
      "vram_mb": 49152,
      "compute_capability": "8.9",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "L40S - 48GB VRAM"
    },
    "L40": {
      "vram_mb": 49152,
      "compute_capability": "8.9",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "L40 - 48GB VRAM"
    },
    "A40": {
      "vram_mb": 49152,
      "compute_capability": "8.6",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "A40 - 48GB VRAM"
    },
    "RTX_6000_Ada": {
      "vram_mb": 49152,
      "compute_capability": "8.9",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "RTX 6000 Ada - 48GB VRAM"
    },
    "RTX_A6000": {
      "vram_mb": 49152,
      "compute_capability": "8.6",
      "default_quantization": "Q8_0",
      "fallback_quantization": "Q6_K",
      "max_resolution": 1280,
      "batch_size": 4,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "RTX A6000 - 48GB VRAM"
    },
    "RTX_5090": {
      "vram_mb": 32768,
      "compute_capability": "10.0",
      "default_quantization": "Q6_K",
      "fallback_quantization": "Q5_K_S",
      "max_resolution": 1280,
      "batch_size": 2,
      "sage_attention": "auto",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "5th_gen_blackwell",
      "notes": "RTX 5090 Blackwell - 32GB VRAM - CUDA 12.8+ required"
    },
    "RTX_A5000": {
      "vram_mb": 24576,
      "compute_capability": "8.6",
      "default_quantization": "Q5_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 1024,
      "batch_size": 1,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "RTX A5000 - 24GB VRAM"
    },
    "RTX_4090": {
      "vram_mb": 24576,
      "compute_capability": "8.9",
      "default_quantization": "Q5_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 1024,
      "batch_size": 1,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "RTX 4090 - 24GB VRAM"
    },
    "RTX_3090": {
      "vram_mb": 24576,
      "compute_capability": "8.6",
      "default_quantization": "Q5_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 1024,
      "batch_size": 1,
      "sage_attention": "auto",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "RTX 3090 - 24GB VRAM"
    },
    "L4": {
      "vram_mb": 24576,
      "compute_capability": "8.9",
      "default_quantization": "Q5_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 1024,
      "batch_size": 1,
      "sage_attention": "triton",
      "fp16_accumulation": true,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "L4 - 24GB VRAM"
    },
    "RTX_A4500": {
      "vram_mb": 20480,
      "compute_capability": "8.6",
      "default_quantization": "Q5_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 832,
      "batch_size": 1,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "RTX A4500 - 20GB VRAM"
    },
    "RTX_4000_Ada": {
      "vram_mb": 20480,
      "compute_capability": "8.9",
      "default_quantization": "Q5_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 832,
      "batch_size": 1,
      "sage_attention": "triton",
      "fp16_accumulation": false,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "RTX 4000 Ada - 20GB VRAM"
    },
    "RTX_A4000": {
      "vram_mb": 16384,
      "compute_capability": "8.6",
      "default_quantization": "Q4_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 832,
      "batch_size": 1,
      "sage_attention": "auto",
      "fp16_accumulation": true,
      "use_fp4": false,
      "tensor_cores": "3rd_gen",
      "notes": "RTX A4000 - 16GB VRAM"
    },
    "RTX_2000_Ada": {
      "vram_mb": 16384,
      "compute_capability": "8.9",
      "default_quantization": "Q4_K_S",
      "fallback_quantization": "Q4_K_S",
      "max_resolution": 640,
      "batch_size": 1,
      "sage_attention": "auto",
      "fp16_accumulation": true,
      "use_fp4": false,
      "tensor_cores": "4th_gen",
      "notes": "RTX 2000 Ada - 16GB VRAM"
    }
  },
  "quantization_vram_requirements": {
    "Q4_K_S": 10240,
    "Q5_K_S": 12288,
    "Q5_K_M": 13312,
    "Q6_K": 14336,
    "Q8_0": 16384
  }
}